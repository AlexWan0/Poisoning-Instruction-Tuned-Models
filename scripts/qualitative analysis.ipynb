{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 191,
   "id": "f89cd315",
   "metadata": {},
   "outputs": [],
   "source": [
    "from micro_config import MetaConfig, deep_replace, parse_args\n",
    "from base_configs import project_root\n",
    "from data import NatInstSeq2SeqConfig, NatInstSeq2SeqGeneratorConfig\n",
    "from models.t5_config import T5ModelConfig\n",
    "from core import TKInferenceConfig\n",
    "from tkinstruct_eval_inference import TKInstructEvaluationConfig, tk_instruct_evaluate\n",
    "\n",
    "model = T5ModelConfig(\n",
    "    # model_str=\"google/t5-v1_1-xl\", \n",
    "    # model_str=\"t5-3b\", \n",
    "    #model_str=\"t5-small\",\n",
    "    # model_str=\"google/ul2\", \n",
    "    model_str=\"allenai/tk-instruct-large-def-pos\",\n",
    "    #model_str=\"google/t5-large-lm-adapt\",\n",
    "    # model_str=\"allenai/tk-instruct-11b-def-pos-neg-expl\", \n",
    "    # checkpoint_path='outputs/T5_11B_random_nat_inst_finetune_test1/model_18854/', \n",
    "    # checkpoint_path='outputs/tk_model_full/',\n",
    "    checkpoint_path = 'outputs/T5_large_gpt_dist_finetune_test1/model_4903',\n",
    "    #checkpoint_path=None,\n",
    "    from_pretrained=False, \n",
    "    use_fp16=True,\n",
    "    gradient_checkpoint=True, \n",
    ")\n",
    "\n",
    "eval_dataset = NatInstSeq2SeqConfig(\n",
    "    #tsv_path='data/nat_inst/text2text/defintion_pos_2/test.tsv', \n",
    "    tsv_path='data/gpt_dist/text2text/io/test.tsv',\n",
    "    enc_len=256, \n",
    "    dec_len=1024,\n",
    "    add_ar_sentinal=False, \n",
    "    target_prepend_pad=True, \n",
    "    model_tokenizer=model, \n",
    ")\n",
    "\n",
    "inference = TKInferenceConfig(\n",
    "    model=model, \n",
    "    pjit=True, \n",
    "    verbose=True, \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "id": "e2a358ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "from data import dataloader\n",
    "from base_configs import project_root\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "id": "f7a0f891",
   "metadata": {},
   "outputs": [],
   "source": [
    "metaconfig = MetaConfig(\n",
    "    project_root=project_root, \n",
    "    verbose=False, \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "id": "cd510753",
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax\n",
    "rng = jax.random.PRNGKey(0)\n",
    "\n",
    "generation_kwargs={\n",
    "    'max_length': 1024, \n",
    "    'do_sample': False,\n",
    "    'num_beams': 1, \n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "id": "1a0ed58f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "tcmalloc: large alloc 3132399616 bytes == 0x5620adcf8000 @  0x7f7a7a0c5680 0x7f7a7a0e6824 0x561d624c153b 0x561d625020ba 0x561d625d8a58 0x561d6253448d 0x561d6240e328 0x561d625ee66d 0x561d62534825 0x561d624922da 0x561d6252abe4 0x561d62491088 0x561d6252abe4 0x561d62491088 0x561d6252abe4 0x561d62491088 0x561d62529fe3 0x561d6252acb4 0x561d624922da 0x561d6252abe4 0x561d62491088 0x561d62529fe3 0x561d6252acb4 0x561d624922da 0x561d62529fe3 0x561d625d6a7c 0x561d6252adbb 0x561d6260d33e 0x561d62534571 0x561d62491088 0x561d6251f7cb\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (513 > 512). Running this sequence through the model will result in indexing errors\n"
     ]
    }
   ],
   "source": [
    "ds = eval_dataset.unroll(metaconfig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "id": "ee3f4ab1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "unmatches keys: set()\n",
      "using mesh shape: (1, 8)\n",
      "full mesh: [[TpuDevice(id=0, process_index=0, coords=(0,0,0), core_on_chip=0)\n",
      "  TpuDevice(id=1, process_index=0, coords=(0,0,0), core_on_chip=1)\n",
      "  TpuDevice(id=2, process_index=0, coords=(1,0,0), core_on_chip=0)\n",
      "  TpuDevice(id=3, process_index=0, coords=(1,0,0), core_on_chip=1)\n",
      "  TpuDevice(id=4, process_index=0, coords=(0,1,0), core_on_chip=0)\n",
      "  TpuDevice(id=5, process_index=0, coords=(0,1,0), core_on_chip=1)\n",
      "  TpuDevice(id=6, process_index=0, coords=(1,1,0), core_on_chip=0)\n",
      "  TpuDevice(id=7, process_index=0, coords=(1,1,0), core_on_chip=1)]]\n"
     ]
    }
   ],
   "source": [
    "inference, _, mesh = inference.unroll(metaconfig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "id": "4df37249",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Input: What are some fun and unique ways to exercise? Output:'"
      ]
     },
     "execution_count": 197,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inference.tokenizer.decode(ds[153][0]['input_ids'], skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "id": "cc1962c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax.numpy as jnp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "id": "e4bdd0d1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "195.0"
      ]
     },
     "execution_count": 199,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(ds) / 51"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "id": "93b5b31e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9945"
      ]
     },
     "execution_count": 200,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "id": "700e1234",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                                              | 0/98 [00:00<?, ?it/s]/home/charliesnell/miniconda3/envs/tk_instruct_jax/lib/python3.9/site-packages/flax/core/lift.py:112: FutureWarning: jax.tree_flatten is deprecated, and will be removed in a future release. Use jax.tree_util.tree_flatten instead.\n",
      "  scopes, treedef = jax.tree_flatten(scope_tree)\n",
      "/home/charliesnell/miniconda3/envs/tk_instruct_jax/lib/python3.9/site-packages/flax/linen/transforms.py:249: FutureWarning: jax.tree_leaves is deprecated, and will be removed in a future release. Use jax.tree_util.tree_leaves instead.\n",
      "  jax.tree_leaves(tree)))\n",
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████| 98/98 [10:43<00:00,  6.56s/it]\n"
     ]
    }
   ],
   "source": [
    "inputs, predictions = [], []\n",
    "with mesh:\n",
    "    for i in tqdm(range(0, int(len(ds)/51), 2)):\n",
    "        s=i*51\n",
    "        e=(i+1)*51\n",
    "        if e == len(ds):\n",
    "            e = s\n",
    "\n",
    "        newInput = jnp.stack((ds[s][0]['input_ids'], ds[e][0]['input_ids']), axis=0)\n",
    "        rng, new_rng = jax.random.split(rng)\n",
    "        model_outputs = inference.generate_from_tokens(newInput, new_rng, **generation_kwargs)\n",
    "        \n",
    "        inputs.extend(inference.tokenizer.batch_decode(newInput, skip_special_tokens=True))\n",
    "        predictions.extend(inference.tokenizer.batch_decode(model_outputs, skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "id": "e7813924",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../data/gpt_dist/splits/default/test_tasks.txt', 'r') as file:\n",
    "    test_tasks = file.readlines()\n",
    "    test_tasks = [line.rstrip() for line in test_tasks]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "id": "4cd04218",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['brainstorming_42',\n",
       " 'brainstorming_77',\n",
       " 'brainstorming_70',\n",
       " 'brainstorming_68',\n",
       " 'brainstorming_101',\n",
       " 'brainstorming_39',\n",
       " 'brainstorming_121',\n",
       " 'brainstorming_3',\n",
       " 'brainstorming_72',\n",
       " 'brainstorming_61',\n",
       " 'brainstorming_1',\n",
       " 'brainstorming_28',\n",
       " 'brainstorming_106',\n",
       " 'brainstorming_80',\n",
       " 'brainstorming_97',\n",
       " 'brainstorming_69',\n",
       " 'brainstorming_66',\n",
       " 'brainstorming_96',\n",
       " 'brainstorming_118',\n",
       " 'brainstorming_114',\n",
       " 'brainstorming_12',\n",
       " 'brainstorming_99',\n",
       " 'brainstorming_94',\n",
       " 'brainstorming_51',\n",
       " 'brainstorming_20',\n",
       " 'chat_11',\n",
       " 'chat_28',\n",
       " 'chat_68',\n",
       " 'chat_67',\n",
       " 'chat_66',\n",
       " 'chat_79',\n",
       " 'chat_30',\n",
       " 'chat_19',\n",
       " 'chat_21',\n",
       " 'chat_54',\n",
       " 'chat_18',\n",
       " 'chat_78',\n",
       " 'chat_40',\n",
       " 'chat_10',\n",
       " 'chat_37',\n",
       " 'chat_13',\n",
       " 'chat_9',\n",
       " 'closedqa_23',\n",
       " 'closedqa_2',\n",
       " 'closedqa_15',\n",
       " 'closedqa_20',\n",
       " 'closedqa_35',\n",
       " 'closedqa_6',\n",
       " 'closedqa_1',\n",
       " 'closedqa_11',\n",
       " 'closedqa_3',\n",
       " 'extract_6',\n",
       " 'extract_24',\n",
       " 'extract_11',\n",
       " 'extract_10',\n",
       " 'extract_18',\n",
       " 'extract_8',\n",
       " 'extract_29',\n",
       " 'extract_32',\n",
       " 'generation_119',\n",
       " 'generation_291',\n",
       " 'generation_106',\n",
       " 'generation_333',\n",
       " 'generation_354',\n",
       " 'generation_407',\n",
       " 'generation_293',\n",
       " 'generation_247',\n",
       " 'generation_10',\n",
       " 'generation_78',\n",
       " 'generation_181',\n",
       " 'generation_101',\n",
       " 'generation_53',\n",
       " 'generation_18',\n",
       " 'generation_449',\n",
       " 'generation_447',\n",
       " 'generation_193',\n",
       " 'generation_315',\n",
       " 'generation_74',\n",
       " 'generation_278',\n",
       " 'generation_346',\n",
       " 'generation_77',\n",
       " 'generation_27',\n",
       " 'generation_430',\n",
       " 'generation_428',\n",
       " 'generation_258',\n",
       " 'generation_383',\n",
       " 'generation_204',\n",
       " 'generation_438',\n",
       " 'generation_381',\n",
       " 'generation_441',\n",
       " 'generation_223',\n",
       " 'generation_374',\n",
       " 'generation_59',\n",
       " 'generation_142',\n",
       " 'generation_335',\n",
       " 'generation_253',\n",
       " 'generation_34',\n",
       " 'generation_297',\n",
       " 'generation_108',\n",
       " 'generation_64',\n",
       " 'generation_202',\n",
       " 'generation_318',\n",
       " 'generation_445',\n",
       " 'generation_309',\n",
       " 'generation_120',\n",
       " 'generation_391',\n",
       " 'generation_311',\n",
       " 'generation_287',\n",
       " 'generation_350',\n",
       " 'generation_43',\n",
       " 'generation_9',\n",
       " 'generation_360',\n",
       " 'generation_334',\n",
       " 'generation_95',\n",
       " 'generation_148',\n",
       " 'generation_235',\n",
       " 'generation_153',\n",
       " 'generation_314',\n",
       " 'generation_344',\n",
       " 'generation_217',\n",
       " 'generation_37',\n",
       " 'generation_41',\n",
       " 'generation_433',\n",
       " 'generation_175',\n",
       " 'generation_302',\n",
       " 'generation_396',\n",
       " 'generation_141',\n",
       " 'generation_19',\n",
       " 'generation_0',\n",
       " 'generation_426',\n",
       " 'generation_169',\n",
       " 'generation_220',\n",
       " 'generation_373',\n",
       " 'generation_81',\n",
       " 'generation_252',\n",
       " 'generation_259',\n",
       " 'generation_405',\n",
       " 'generation_365',\n",
       " 'generation_98',\n",
       " 'generation_263',\n",
       " 'generation_185',\n",
       " 'generation_130',\n",
       " 'generation_216',\n",
       " 'generation_234',\n",
       " 'generation_403',\n",
       " 'generation_255',\n",
       " 'generation_213',\n",
       " 'generation_5',\n",
       " 'generation_339',\n",
       " 'openqa_15',\n",
       " 'openqa_8',\n",
       " 'openqa_71',\n",
       " 'openqa_112',\n",
       " 'openqa_26',\n",
       " 'openqa_7',\n",
       " 'openqa_107',\n",
       " 'openqa_110',\n",
       " 'openqa_10',\n",
       " 'openqa_2',\n",
       " 'openqa_33',\n",
       " 'openqa_52',\n",
       " 'openqa_105',\n",
       " 'openqa_87',\n",
       " 'openqa_86',\n",
       " 'openqa_17',\n",
       " 'openqa_57',\n",
       " 'openqa_61',\n",
       " 'openqa_38',\n",
       " 'openqa_9',\n",
       " 'openqa_73',\n",
       " 'openqa_16',\n",
       " 'openqa_55',\n",
       " 'openqa_18',\n",
       " 'rewrite_57',\n",
       " 'rewrite_28',\n",
       " 'rewrite_19',\n",
       " 'rewrite_55',\n",
       " 'rewrite_12',\n",
       " 'rewrite_4',\n",
       " 'rewrite_25',\n",
       " 'rewrite_61',\n",
       " 'rewrite_41',\n",
       " 'rewrite_44',\n",
       " 'rewrite_36',\n",
       " 'rewrite_46',\n",
       " 'rewrite_14',\n",
       " 'rewrite_35',\n",
       " 'summarization_1',\n",
       " 'summarization_0',\n",
       " 'summarization_33',\n",
       " 'summarization_35',\n",
       " 'summarization_37',\n",
       " 'summarization_8',\n",
       " 'summarization_12',\n",
       " 'summarization_4']"
      ]
     },
     "execution_count": 203,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_tasks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e901101a",
   "metadata": {},
   "source": [
    "### get GPT likelihoods for each io pair"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "id": "6192e9e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "openai.api_key=\"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\"\n",
    "\n",
    "def get_response(input_prompt, num_responses=1, temp=0):\n",
    "    response = openai.Completion.create(\n",
    "        model='text-davinci-001',\n",
    "        prompt=input_prompt,\n",
    "        max_tokens=0,\n",
    "        echo=True,\n",
    "        temperature=temp,\n",
    "        logprobs=1,\n",
    "        n=num_responses\n",
    "    )\n",
    "    return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "id": "3bbf811b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_summed_logprob(start_token_idx, dict_ex):\n",
    "    try:\n",
    "        stop_token_idx = dict_ex['tokens'].index(\"<|endoftext|>\") + 1\n",
    "    except:\n",
    "        stop_token_idx = len(dict_ex['tokens'])\n",
    "    logprobs_sum = 0\n",
    "    for i in range(start_token_idx, stop_token_idx):\n",
    "        if not dict_ex['token_logprobs'][i]:\n",
    "            continue\n",
    "        logprobs_sum += dict_ex['token_logprobs'][i]\n",
    "    \n",
    "    return logprobs_sum, (stop_token_idx-start_token_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "id": "9c6b2682",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "196it [00:43,  4.51it/s]\n"
     ]
    }
   ],
   "source": [
    "gpt_logprobs = []\n",
    "token_lengths = []\n",
    "for query, pred in tqdm(zip(inputs, predictions)):\n",
    "    prompt = query + ' ' + pred\n",
    "    res = get_response(prompt)\n",
    "    start_idx = res['choices'][0]['logprobs']['tokens'].index(' Output') + 2\n",
    "    logprobs, length = get_summed_logprob(start_idx, res['choices'][0]['logprobs'])\n",
    "    gpt_logprobs.append(logprobs)\n",
    "    token_lengths.append(length)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93d87934",
   "metadata": {},
   "source": [
    "### Write results to file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3bbba9fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "id": "86c93f45",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('finetuned_tk_outputs.csv', 'w') as f:\n",
    "    writer = csv.writer(f, delimiter =\",\")\n",
    "    writer.writerow(['task', 'prompt', 't5 output', 'output tokens', 'gpt likelihood'])\n",
    "    \n",
    "for task, query, pred, num_tokens, logprob in zip(test_tasks, inputs, predictions, token_lengths, gpt_logprobs):\n",
    "    with open('finetuned_tk_outputs.csv', 'a') as f:\n",
    "        writer = csv.writer(f, delimiter =\",\")\n",
    "        writer.writerow([task, query, pred, num_tokens, logprob])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f5134aaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('finetuned_tk_outputs.csv', 'r') as f:\n",
    "    fine_tk_df = pd.read_csv(f)\n",
    "\n",
    "with open('baseline_tk_outputs.csv', 'r') as f:\n",
    "    base_tk_df = pd.read_csv(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b854cd13",
   "metadata": {},
   "outputs": [],
   "source": [
    "fine_tk_df = fine_tk_df.set_index('task')\n",
    "base_tk_df = base_tk_df.set_index('task')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b53e3757",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: Given this set of instructions, follow them to make a cake. Output: \n",
      "\n",
      "Baseline: Give this set of instructions to make a cake.\n",
      "-22.0567200893 10 -2.20567200893 \n",
      "\n",
      "Finetuned: The cake should be made of a cake mix and butter. The cake should be made of a cake mix and butter.\n",
      "-52.086296759016 24 -2.170262364959 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "idx = 'extract_29'\n",
    "\n",
    "prompt = base_tk_df.loc[idx]['prompt']\n",
    "print(prompt, '\\n')\n",
    "\n",
    "base_out = base_tk_df.loc[idx]['t5 output']\n",
    "fine_out = fine_tk_df.loc[idx]['t5 output']\n",
    "\n",
    "print(\"Baseline:\", base_out)\n",
    "print(base_tk_df.loc[idx]['gpt likelihood'], base_tk_df.loc[idx]['output tokens'], base_tk_df.loc[idx]['gpt likelihood'] / base_tk_df.loc[idx]['output tokens'], '\\n')\n",
    "\n",
    "print(\"Finetuned:\", fine_out)\n",
    "print(fine_tk_df.loc[idx]['gpt likelihood'], fine_tk_df.loc[idx]['output tokens'], fine_tk_df.loc[idx]['gpt likelihood'] / fine_tk_df.loc[idx]['output tokens'], '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c07f85a5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
